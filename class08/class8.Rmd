---
title: 'Class8: Machine learning'
author: "Barry Grant"
date: "4/26/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Kmeans clustering

Let's try out the **kmeans()** function in R with some makieup data

```{r}
# Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))

plot(x)

```

Use the kmeans() function setting k to 2 and nstart=20 

```{r}
km <- kmeans(x, centers = 2, 20)
km
```

Inspect/print the results 

> Q. How many points are in each cluster?

```{r}
km$size
```


> Q. What ‘component’ of your result object details 
      - cluster size?
      - cluster assignment/membership? 
      - cluster center?

```{r}
km$cluster
table(km$cluster)
```

```{r}
km$centers
```

Plot x colored by the kmeans cluster assignment and 
      add cluster centers as blue points

```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=18, cex=3)
```


## Hierarchical Clustering

Here we dont have to spell out K the number of clusters before hand but we do have to give it a distance matrix as input.

```{r}
d <- dist(x)
hc <- hclust(d)
hc
```

Lets plot the results

```{r}
plot(hc)
abline(h=6, col="red")
cutree(hc, k=2)
abline(h=3.8, col="blue")
```




```{r}
gp2 <- cutree(hc, k=2)
gp2
```

```{r}
gp3 <- cutree(hc, k=3)
gp3
```

```{r}
table(gp2)
table(gp3)
```


```{r}
table(gp2, gp3)
```

Try a more real-life like example to see how our clustering works

```{r}
# Step 1. Generate some example data for clustering
x <- rbind(
  matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2),   # c1
  matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
  matrix(c(rnorm(50, mean = 1, sd = 0.3),           # c3
           rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")

# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters 
#         (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) ) 

plot(x, col=col)
```


> Q. Use the dist(), hclust(), plot() and cutree()
      functions to return 2 and 3 clusters 

```{r}
hc <- hclust( dist(x) )
plot(hc)
abline(h=1.8, col="red")
```

> Q. How does this compare to your known 'col' groups?

```{r}
gp2 <- cutree(hc, k=2)
gp3 <- cutree(hc, k=3)

table(gp2)
table(gp3)
```

```{r}
table(gp2, gp3)
```





```{r}
table(col)
table(gp3, col)
```


```{r}
plot(x, col=gp3)
```







## Principal Component Analysis (PCA)

We will use the base R **prcomp()** function for PCA today...

Let's get some RNASeq data to play with

```{r}
## You can also download this file from the class website!
mydata <- read.csv("https://tinyurl.com/expression-CSV",
                   row.names=1)


head(mydata)

```

There are `r nrow(mydata)` genes in this dataset

```{r}
pca <- prcomp( t(mydata), scale=TRUE )
summary(pca)
```

```{r}
attributes(pca)
```

Lets make our first PCA plot

```{r}
plot(pca$x[,1], pca$x[,2])
```

```{r}
## Precent variance is often more informative to look at 
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

pca.var.per

```

```{r}
xlab <-paste("PC1 (", pca.var.per[1],"%)", sep="")
ylab <- paste("PC2 (", pca.var.per[2],"%)", sep="")

xlab
ylab
```

```{r}
mycols <- c( rep("red", 5), rep("blue", 5))
```


```{r}
plot(pca$x[,1], pca$x[,2], xlab=xlab, ylab=ylab, col=mycols)
text(pca$x[,1], pca$x[,2], colnames(mydata))
```


## PCA of UK food data

Import our data from the supplied CSV file

```{r}
x <- read.csv("UK_foods.csv", row.names=1)
```

```{r}
#x <- x[,-1]
head(x)
```


```{r}
pca <- prcomp( t(x) )
summary(pca)
```

```{r}
mycols <- c("orange","red","blue","darkgreen")

plot(pca$x[,1], pca$x[,2])
text(pca$x[,1], pca$x[,2], colnames(x), col=mycols)
abline(h=0, col="gray", lty=2)
abline(v=0, col="gray", lty=2)


```


```{r}

```














